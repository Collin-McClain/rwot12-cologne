
# Sustainable Privacy, Authenticity, and Confidentiality

Author: Samuel M. Smith Ph.D.

## Introduction

In the [SPAC whitepaper](https://github.com/SmithSamuelM/Papers/blob/master/whitepapers/SPAC_Message.md),  a protocol is described using best practices with readily available cryptographic operations to provide secure privacy, authenticity, and confidentiality.  The protocol provides a foundation for best practices for what we are calling ***sustainable privacy***.  We intend to further define, explicate, and develop those practices in light of the technological, legal, and economic challenges to sustainable privacy.

## Time Value of Information

In general, privacy dissipates over time. This is because digital information is inherently leaky, and those leaks become more correlatable as the body of leaks grows over time. This leakiness can be balanced by the diminishing exploitable time value of correlated information. 

In general, the primary time value of correlated information for data aggregators is that it can be used to predict behavior. Advertisers want to predict who will most likely be receptive to their marketing campaigns. The predictive accuracy of aggregated behavioral information of potential participants in any given market is largely a function of the nearness in time of the behavior used to make the prediction. We can ascribe a time constant to a given market's exploitable predictive potential where information older than the time constant no longer has net predictive value in excess of the cost of aggregating it. Information that exceeds this time constant is considered stale because there is no longer any incentive to aggregate and correlate it. Therefore, in spite of the fact that privacy dissipates over time, the value of correlation also diminishes over time so that cost-effective privacy protection mechanisms can focus resources on near-term correlatability. This provides a sweet spot for sustainable privacy protection that is governed by the time constant of the time value of exploitable correlation. Likewise, the cost of privacy protection can be weighed against the cost due to the harm of exploitation.  If the cost of protection exceeds the cost due to the harm of exploitation, then it's not worth protecting (i.e., it's counterproductive to the protector). If the cost of correlation in order to exploit exceeds the time value return of exploitation, then it's not worth exploiting (i.e., it's counterproductive to the exploiter).

The protocols defined in the (SPAC whitepaper)[https://github.com/SmithSamuelM/Papers/blob/master/whitepapers/SPAC_Message.md] provide mechanisms for granular partitioning contexts so that correlatability is also granularly partitioned. This enables one to control the exploitable time value of the correlatable information that can be leaked out of that context. Once a context has become leaky, however, a new isolated context can be created that restarts the clock on time value correlatability. This provides a trade-space between the friction and cost of forming and maintaining contexts, the length of time before a given context becomes leaky, and the time constant of the exploitable value of leaked information. The cost of protection includes expensive one-time OOBA setups. If the leakiness of a given context is cost-effectively protectable beyond the time constant on the time value of exploitation, then new information is sustainably protectable indefinitely.

## Sustainable Management of Contexts

A party wishing to protect against exploitation via correlatable metadata (identifiers) sustainably has a similar problem to that of the operatives in covert and/or clandestine operations. 

***Covert*** 
A covert operation is an operation that is planned and executed in secrecy so that the identity of the agency or organization remains unknown or is plausibly deniable.  

***Clandestine*** 
A clandestine operation is an operation that is carried out in such a manner that the operation remains in secrecy or is concealed.

(see [DoD Dictionary](https://irp.fas.org/doddir/dod/dictionary.pdf), [Covert vs. Clandestine](https://www.differencebetween.com/difference-between-covert-and-vs-clandestine/))

From the standpoint of a covert operative, the fact that the organization conducting the operation remains unknown puts the operative at great risk. The organization can burn or disavow the operative without incurring liability to the organization itself. This means the operative must have a legitimate public cover identity. Whereas a clandestine operative has no such public cover, should the operative be discovered then that discovery would incur liability to the organization itself. This means clandestine operations have more limited use cases. Often constrained but the technical and operative challenges of maintaining the privacy of the whole operation, including all operatives. Consequently, covert operations have more use cases that come at the cost of setting up public cover identities for the operatives.

As a result, A covert operative must be able to survive long-term without being found out. This means they must have a public cover identity that masks the private behavior they wish to hide. Effectively, their public cover identity allows them to hide other private behavior in plain sight. In comparison, a clandestine operative has no such public cover but must operate totally privately. Any leakage at all by a clandestine operative of its private behavior becomes a strong signal that can be correlated to detect their operation. As a result, a given clandestine operation is unsustainable long-term. They tend to be short-term, in and out. In contrast, covert operations can be much more long-term because the leakiness of private behavior is masked by their public cover behaviors. 

The analogy for our purposes is that many approaches to internet privacy have tended to look like clandestine operations where the whole operation must be concealed and that concealment is largely technological. But any leakage at all jeopardizes the whole operation. This makes the approach fragile and largely unsustainable. In comparison, a more covert operation approach may be more robust to leakage and hence more sustainable.  For example, there are various ways to provide the equivalent of a "cover" identity for communications traffic. Specifically, with regard to the protocols above, parties *A* and **B** have several relationship contexts. They each have a hop-wise communications context with an intermediary, e.g. *A* with *C* and *B* with *D*. To the extent that *C* and *D* have many users that have well-known legitimate "public" uses of *C* and *D*, then *C* and *D* each have a cover identity with respect to their respective relationships with *A* and *B*. For example, if the only time anyone uses *C* is to conduct some private activity, then the use of *C* by itself is suspect and provides correlatable information.  Specifically, suppose that *C* is a VPN, and the only time *A* uses *C* is to communicate with a crypto-currency exchange in Cyprus in order to avoid the FATF KYC rules for exchanges in FATF-compliant countries then, anytime *A* uses *C* the use is exploitable information. Whereas if *A* uses *C* for all communications, then the occasional communication to Cyprus is concealed by the cover of the other activity. Likewise, if all users of *C* use *C* for all their traffic, then they provide herd privacy to every other user. But if most of the users of *C* only use *C* to conceal transactions for which they want more privacy (i.e., clandestine), they make the mere use of *C* itself a correlatable signal that exposes their clandestine operation. 

The nesting of contexts extends the covert analogy. Given *A* has a generic end-wise routing relationship context with *B*, that *A* uses for a multitude of confidential interactions with *B* where each interaction uses a dedicated interaction relationship context between *A* and *B.*, then the end-wise routing context provides cover for the end-wise interaction contexts. The traffic over the general routing context is another form of herd privacy. This enables *A* *and *B* to conduct sustainable hide-in-plain-sight operations that minimize the exploitable correlatable information. Each of the relatively public long-term generic communication contexts (covert) and long-term generic routing contexts (covert) provide nested covers to the really valuable private short-term interaction contexts (clandestine).

## Re-Identification

To elaborate on the futility of purely clandestine methods for sustainable privacy, consider the problem of de-identification and re-identification of data. It was long thought that de-identification or anonymization of data could provide privacy using a technique called (k-anonymity)[https://en.wikipedia.org/wiki/K-anonymity]. Recent research has shown that fully de-identified sparse datasets can be merged to re-identify the data (re-identification)[https://www.nature.com/articles/s41467-019-10933-3/] (Lie of Anonymous Data)[https://techcrunch.com/2019/07/24/researchers-spotlight-the-lie-of-anonymous-data/]. In 2022, this was further extended to what is called a down-coding attack which enables the re-identification of data even when every field is a quasi-identifier, i.e., there is no personally identifying information in the dataset, (down-coding attack)[https://www.usenix.org/system/files/sec22-cohen.pdf].  Indeed the ease and pervasiveness by which de-identified data may be re-identified have resulted in the US FTC (Federal Trade Commission) issuing a warning that those who share de-identified databases and purport that merely through de-identification that the privacy rights of the associated persons are protected may be in violation of the laws regulating the use and sharing of sensitive data. (FTC Illegal Use and Sharing)[https://www.ftc.gov/business-guidance/blog/2022/07/location-health-and-other-sensitive-information-ftc-committed-fully-enforcing-law-against-illegal] 

## Contextual Linkability Re-identification Attack

Furthermore, both the cryptographic unlinkability via ZKPs and the selective disclosure of partial data (whether via ZKP or not) of any 1st party data disclosed to a 2nd party may be potentially trivially contextually linkable via re-identification correlation techniques such as a down-coding attack. This means that merely clandestine mechanisms such as zero-knowledge-proofs or selective disclosure mechanisms that provide so-called cryptographic unlinkability may now be trivially linkable by statistical methods. We call this attack, contextual linkability. Therefore, the only viable protection against 2nd party correlation is pre-disclosure contractual protection by imposing liability on the 2nd party disclosee and strict post-disclosure chain-link confidentiality maintained by the 2nd party disclosee and any downstream disclosees or other users of that data including any assimilation or aggregation. The core problem of any k-anonymity-like approach is there is no apriori way to establish if any attribute is an identifier, quasi-identifier, or non-identifier because all attributes are potentially identifying based on the available auxiliary data. As a result, the de-identification of aggregated 1st party data provides no meaningful privacy protection. Hence, 2nd parties may not aggregate 1st party de-identified or not without the permission of the 1st party. To reiterate, Bare (naive) selective disclosure alone provides no guarantee of privacy protection to the discloser. The only sustainable privacy protection mechanism of 1st party data disclosed to a 2nd party, even when selectively disclosed, (via ZKP or not) is contractual disclosure with chain-link confidentiality because any selective disclosure is potentially ineffective unless performed within the confines of a contractually protected disclosure that imposes an incentive on the disclosee (verifier) to protect that disclosure (counter-incentive against the exploitation of that disclosure). 

To further elaborate, as mentioned above, any de-identified dataset, even when all attributes are part of quasi-identifiers may be vulnerable to re-identification through various statistical attacks such as a down-coding attack. Because contextual linkability has the potential to create a data set of quasi-identifiers that may be combined with selectively disclosed attributes in such a way as to re-identify the associated subject of the selectively disclosed attributes, ZKPs or other selective disclosure mechanisms, by themselves are insufficient privacy protection mechanisms. The verifier may structure the context of the presentation so as to provide sufficient auxiliary data that the combination of contextual auxiliary data and selectively disclosed data is identifying. 

This limitation relegates the status of selective disclosure and/or ZKPs as privacy mechanisms to the narrow corner conditions where there is zero contextual linkability by the 2nd party disclosee (verifier) at the time of presentation. Because most presentation contexts are under the control of the 2nd party (verifier), the verifier needs merely to structure that context with enough quasi-identifier attributes (auxiliary data) to re-identify the presenter which in turn would enable the 2nd party to link the de-identified presenter back to the issuer with the presentation details thereby defeating unlinkability which is the only unique reason to use a ZKP in the presentation in the first place. This does not mean that there are no corner conditions where the presentation context is sufficiently under the control of the presenter (1st) party such that the presenter can structure the context so as to prevent the verifier from correlating any other quasi-identifiers, but none of the standard use cases satisfy that condition.

In light of this vulnerability, many of the standard use cases for selective disclosure and/or ZKPs in VCs are examples of anti-patterns for privacy protection. This is because these standard use cases assume a presentation context that is under the control of the verifier, which means a smart verifier can restructure that context to statistically guarantee correlation and defeat the selective disclosure and/or ZKP.

Correctly understood, selective disclosure is a naive form of k-anonymity performed by the discloser (presenter). The discloser is attempting to de-identify their own data.  Unfortunately, such naive de-identified disclosure is not performed with any statistical insight into the ability of the verifier (receiver) to re-identify the selectively disclosed attributes given the contextual attributes that are also disclosed (inadvertently) at the time of presentation and under the control of the verifier. For example, when a disclosure is made at the place of business of the receiver (verifier), the receiver may use readily available location data from mobile phone providers to de-identify based on a geo-fence query. Likewise, when the disclosure is made to the website of the receiver, the receiver may use readily available IP source addressing and routing information to re-identify the presenter. If payment is required, then the receiver may use credit card information to re-identify the presenter. If a facial biometric is required, then the receiver may use readily available facial recognition systems to re-identify the presenter. Likewise for on-premise security cameras.  

Indeed, systems that use selective disclosure as an advertised privacy protection mechanism may result in a net decrease in privacy protection for users because of the false belief that selective disclosure alone is sufficient to protect the disclosure. This may induce an increase in re-identifiable disclosures that would not have happened otherwise. Users may be lulled into a false belief that because their selectively disclosed attributes do not include any personally identifying information that there is no need for them to impose any constraints on the use and sharing of their naively de-identified (selectively disclosed) attributes. Indeed, the receiver may surreptitiously induce such unconstrained disclosures by reinforcing the false belief that the de-identified attributes are not (easily) re-identifiable. For this reason, the use of a bare selective disclosure mechanism may be considered irresponsible by any organization that purports to use it for privacy protection.
